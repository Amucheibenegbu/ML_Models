{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faae427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def read_data(run_num):\n",
    "    #Source:  Pima-Indian diabetes dataset: https://www.kaggle.com/kumargh/pimaindiansdiabetescsv\n",
    "\n",
    "    data = pd.read_csv(\"pima-indians-diabetes.csv\", sep=\",\", header = None)\n",
    "    X = data.iloc[:,0:8].values\n",
    "    y = data.iloc[:, -1].values\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.4,stratify=y, random_state=run_num)\n",
    "    scaler = StandardScaler()\n",
    "    x_train_s = scaler.fit_transform(x_train)\n",
    "    x_test_s = scaler.transform(x_test)\n",
    "\n",
    "    return x_train_s, x_test_s, y_train, y_test\n",
    "\n",
    " \n",
    "    \n",
    "def logistic(x_train, x_test, y_train, y_test, l1_ratio=0.5):\n",
    "    #Source: Scikit Learn. (n.d). Linear Regression Example. https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html \n",
    "    penalties= ['l1','l2','elasticnet']\n",
    "    results = {}\n",
    "\n",
    "    for pen in penalties:\n",
    "        if pen == 'elasticnet':\n",
    "            name = f\"elasticnet_{l1_ratio:.2f}\"\n",
    "            model = linear_model.LogisticRegression(penalty=pen, tol=0.01,solver='saga', l1_ratio=l1_ratio)\n",
    "        else:\n",
    "            name = pen\n",
    "            model = linear_model.LogisticRegression(penalty=pen, tol=0.01,solver='saga') #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        y_train_preds = model.predict(x_train)\n",
    "        y_test_preds = model.predict(x_test)\n",
    "        y_train_probs = model.predict_proba(x_train)[:, 1]\n",
    "        y_test_probs = model.predict_proba(x_test)[:, 1]\n",
    "        results[name] = {\n",
    "            \"y_train_pred\": y_train_preds,\n",
    "            \"y_test_pred\":  y_test_preds,\n",
    "            \"y_train_prob\": y_train_probs,\n",
    "            \"y_test_prob\":  y_test_probs\n",
    "        }\n",
    "        \n",
    "    return results\n",
    "\n",
    "def metrics(y_train, y_test, results):\n",
    "    rows = []\n",
    "    outputs = {}\n",
    "    for name, res in results.items():\n",
    "        ytrp, ytep = res[\"y_train_pred\"], res[\"y_test_pred\"]\n",
    "        ytrb, yteb = res[\"y_train_prob\"], res[\"y_test_prob\"]\n",
    "        acc_train = accuracy_score(y_train, ytrp)\n",
    "        acc_test = accuracy_score(y_test, ytep)\n",
    "        auc_train = roc_auc_score(y_train, ytrb)\n",
    "        auc_test = roc_auc_score(y_test, yteb)\n",
    "        f1_train = f1_score(y_train, ytrp)\n",
    "        f1_test = f1_score(y_test, ytep)\n",
    "        fpr, tpr, _ = roc_curve(y_test, yteb)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, yteb)\n",
    "    \n",
    "        outputs[name] = {\n",
    "            'AUC_test': auc_test, 'F1_test': f1_test,\n",
    "            'FPR': fpr,'TPR': tpr,\n",
    "            'Precision': precision,'Recall': recall}\n",
    "        rows.append([name, acc_train, acc_test, auc_train, auc_test, f1_train, f1_test])\n",
    "\n",
    "    table = pd.DataFrame(rows,\n",
    "    columns=[\"Penalty\", \"ACC_train\", \"ACC_test\", \"AUC_train\", \"AUC_test\", \"F1_train\", \"F1_test\"]\n",
    "    ).set_index(\"Penalty\")\n",
    "    return table, outputs\n",
    "\n",
    "def plot_curves(outputs):\n",
    "    plt.figure()\n",
    "    for name, m in outputs.items():\n",
    "        auc_val = m[\"AUC_test\"]\n",
    "        label = f\"{name} (AUC: {auc_val:.2f})\"\n",
    "        plt.plot(m[\"FPR\"], m[\"TPR\"], marker='.',linewidth=1.5, label=label) \n",
    "    plt.plot([0, 1], [0, 1], \"k--\", alpha=0.5)    \n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('AUC_plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    for name, m in outputs.items():\n",
    "        label = f\"{name} (F1: {m['F1_test']:.2f})\"\n",
    "        plt.plot(m['Recall'], m['Precision'], marker='.', linewidth=1.5, label=label)\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('Recall_precision_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def cross_validating(X, y, l1_ratio=0.5):\n",
    "    model = linear_model.LogisticRegression(C=1.0, penalty='elasticnet', tol=0.01, solver='saga', l1_ratio=l1_ratio)\n",
    "    \n",
    "    scoring = ['roc_auc', 'f1']\n",
    "    scores = cross_validate(model, X, y, cv=10, scoring=scoring, return_train_score=True)\n",
    "    \n",
    "    roc_auc = scores['test_roc_auc']\n",
    "    f1 = scores['test_f1']\n",
    "    return roc_auc, f1\n",
    "\n",
    "def summary_metrics(table):\n",
    "    # 1) Stack all run tables into one big table\n",
    "    all_runs = pd.concat(table, axis=0)  # rows just pile up\n",
    "    # 2) Group by model name (the index) and compute mean & std\n",
    "    means = all_runs.groupby(all_runs.index).mean()\n",
    "    stds  = all_runs.groupby(all_runs.index).std()\n",
    "\n",
    "    # 3) Build a numeric summary with *_mean and *_std columns\n",
    "    summary = pd.DataFrame(index=means.index)\n",
    "    for col in means.columns:\n",
    "        summary[f\"{col}_mean\"] = means[col]\n",
    "        summary[f\"{col}_std\"]  = stds[col]\n",
    "\n",
    "    # 4) Build a pretty table with \"mean ± std\" strings\n",
    "    result_df = pd.DataFrame(index=means.index)\n",
    "    for col in means.columns:\n",
    "        result_df[col] = means[col].round(3).astype(str) + \" ± \" + stds[col].round(3).astype(str)\n",
    "\n",
    "    return result_df, summary\n",
    "\n",
    "def main():\n",
    "    runs = 10\n",
    "    all_results = []\n",
    "    for run in range(runs):\n",
    "        x_train, x_test, y_train, y_test = read_data(run)\n",
    "        results = logistic(x_train, x_test, y_train, y_test)\n",
    "        table, output = metrics(y_train, y_test, results)\n",
    "        all_results.append(table)\n",
    "        if run == runs - 1:\n",
    "            plot_curves(output)\n",
    "    result_df, summary = summary_metrics(all_results)\n",
    "    print(all_results[-1].round(3))\n",
    "    print(result_df)\n",
    "    roc_auc, f1 = cross_validating(x_train, y_train)\n",
    "    print(f\"ROC AUC mean and STD: {np.mean(roc_auc):.3f} ± {np.std(roc_auc):.3f}\")\n",
    "    print(f\"F1 mean and STD: {np.mean(f1):.3f} ± {np.std(f1):.3f}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
